Capitolo 9 di Ullman massive datasets
Reccomendation system sono un insieme di modelli e metodi, è una clase di applicazioni(web based ) che predicono le risposte degli utenti slla base delle loro preferneze(risposte degli utenti sulla base delle loro azioni, cosa piace all'utente, scopo veramente economico)
esempi sono quotidiani online, articoli sullo stesso sito web, raccomandati secondo modelli predittivi, anticipazione o immissione di un interesse.
sono nati per aumentare le entrate in denaro

arriva un utente per un acquisto di cd, ha acquistato e ascolta altri cd, quello che vorremmo fare è apprendere le sue preferenze, arriva un'altro cliente Y che acquista le stesse cose, allora gli suggeriamo quello che ha acquistato l'altro.

content-based : esaminare le proprietà degli articoli per raccomandare nuovi
collaborative filtering: similarità tra gli utenti per raccomandare nuovi prodotti

Abbiamo due classi di entità:
	insieme di utenit
	insieme di oggetti

rappresentate all'interno di una utility matrix,  matrice sparsa che per ogni coppia utente oggetto calcola un grado du preferenza dell'utente per l'oggetto
obiettivo del sistema di raccomandazione è predire le entry vuote della matrice per inferire le preferenze dell'utente, la matrice di Netflix è molto sparsa(98%)
lo 0 viene considerato come la non interazione con l'oggetto
dovremmo scartare utenti che danno sempre lo stesso voto(?)
troviamo dei pattern negli interessi degli utenti, cerchiamo di predire gli interessi

fenomeno della long tail: i negozi fisici hanno spazio limitato,mostrano gli oggetti più in voga, il web(teoricamente) non ha questo problema.

problemi: come collezioniamo le valutazioni per la utility matrix, come ignoro certi recort

collezione: esplicito con le valutazioni esplicite degli utenti, il professore dice che le persone mentono, tutti vogliono uccidersi, implicito, sulla base delle azioni implicite dell'utente, sono tutti e due soggetti a spam(rating-farms)

RACCOMANDAZIONI CONTENT BASED:

idea principale: ad un utente piacciono certi tipi di oggetti, costruiamo un profilo utente in base alla caratteristiche degli oggetti a cui l'utente è interessato, raccomandiamo i match dal profilo utente(esempio raccomandazione sul tipo di film), abbiamo due vettori, uno del profilo utente e uno sulle proprietà dell'oggetto da raccomandare, calcolo la quantità di match e raccomando se supera una certa soglia
per ogni oggetto si crea un profilo(insieme di feature che rappresentano caratteristiche rilevanti), ci sono degli oggetti che non si adattano bene a questo modello(documenti, cosa scegliamo come caratteristiche, se usiamo TF.IDF(frequenza parola(termine) documento/ frequenza parola(termine) in tutti i documenti))

come facciamo a costruire il profilo dell'utente,usiamo la media ponderata del profilo degli oggetti a cui è interessato(o usiamo un altro tipo sempre usufruendo dei suoi interessi), usiamo euristiche di predizione per predire gli oggetti da raccomandare? 

COLLABORATIVE FILTERING:

tutti gli utenti collaborano alla costruzione del modello di raccomandazione, troviamo utenti che hanno rating simili.
come valutiamo la similarità degli utenti, possiamo usare le misure di similarità(jaccard o cosine o pearson o qualsiasi cosa che misuri la similarità)
problema con molti utenti, troppe distanze da calcolare, un modo per risolvere è KNN , prediciamo l'oggetto usando la similarità con altri utenti(simil media pesata usando la similarità)
possiamo rigirare le carte vedendo gli oggetti simili (utilizziamo le righe della matrice utility al posto delle colonne nel calcolo della similarità e del modello di predizione) e calcolando il rating, questo perchè il numero di oggetti è solitamente minore rispetto al numero di utenti. per la similarità e il rating conseguente usiamo la cosa delle slide che non scrivo(baseline come la media dei rating per normalizzare i rating delle predizioni).

pro e contro nelle slide
quando abbiamo una matrice sparsa al posto di valutare oggetti usiamo categorie di oggetti(comprimiamo gli oggetti), con questo intendiamo ridurre la dimensionalità(in questo modo riduciamo la sparsità della matrice), LSI per CF (gli oggetti vengono mappati-associati a concetti, e di conseguenza abbiamo pure una mappattura inversa da concetti a oggetti, oggetti<->categorie,utenti<->categorie)
SVD(singualar value decomposition) è una riduzione di dimensionalità, costruita in modo di essere riportata alla dimensionalità originaria.
usiamo proprietà algebriche delle matrici(guarda le slide per definizione e descrizione)
la matrice lambda ha gli autovalori sulla diagonale(ordinati dal più grande al più piccolo)
decomposizione spettrale

VALUTAZIONE DEI RISULTATI:
dobbiamo valutare i nostri modelli, come facciamo(RMSE, coverage, precision, curve ROC)