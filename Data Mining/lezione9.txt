l'altra volta avevamo discusso delle strategie per la creazione di un modello decisionale, ora stiamo parlando di pruning(non mi ricordo se era effettivamente questo quello di cui avevamo parlato l'ultima volta)

troppi attributi in un training set possono fare crescere la dimensione dell'albero in modo spropositato, ciò può portare alla costruzione di un modello overfittato, basato troppo sul training set, costruito ad hoc per lui, con poca flessibilità e capace di classificare in modo sbagliato nuovi dati.
Per evitare overfitting usiamo la tecnica di pruning, cioè togliamo i rami dell'albero che non contribuiscono ad una corretta classificazione, in questo modo non stiamo migliorando solo l'accuratezza del nostro algoritmo, ma anche la sua comprensibilità(e talvolta pure la velocità).
Per attuare il pruning, puntiamo a diminuire il tasso di errore di classificazione del modello  

PRUNING	:
pre- pruning: nel mentre costruiamo l'albero, decidiamo come splittare e se splittare ancora, si usa la misura di gain, il guadagno deve superare una certa soglia, in alternativa il troncamento può essere fatto con metodi statistici.
post- pruning: alla fine, si usano diversi metodi (C4.5 e CART), nonostante sia più dispendioso, C4.5 e CART usano questo approccio(il professore dice che lo usiamo pure perchè è piu facile da implementare)

il pre  pruning può essere più potente ma anche più difficile implementare, quindi siccome siamo schifosi facciamo la cosa peggiore cioè il post pruning

il pruning pessimistico è basato sull'error rate(tuple classificate in modo sbagliato rispetto a tutto, o una parte, l'insieme di tuple), usiamo l'error rate per vedere quante tuple sono classificate in maniera errata.
valutiamo la variazione di error rate e vediamo se è positiva(aumento di errore) o negativa(diminuzione di errore), agendo di conseguenza potando l'albero e sostituendo con foglie

si chiama pessimistico perchè calcoliamo una stima pessimistica, sovrastimiamo epsilon, in pratica se l'error rate è al limite del threshold stabilito, il ramo viene comunque tagliato, stimiamo l'errore atteso sul training set prima e dopo lo splitting del nodo x, vedendo se siano tra loro maggiori o minori, se la stima dopo è maggiore allora sostituiamo il sottoalbero con un nodo foglia ed assegnamo a x l'etichetta di maggioranza tra le foglie del vecchio sottoalbero(quindi si fa una stima dell'errore prima e dopo lo split, si confrontano i due valori e si decide di conseguenza)
sulle slide per vedere cosa intende con stima pessimistica. quindi per il dopo split usa un k per aumentare o diminuire epsilon


il CART si basa sullo stesso principio, solo che in questo caso memorizzo tutti gli alberi decisionali che posso ottenere, T0 è l'albero completo, Tn l'albero solo con una radice(il più complex), gli alberi rimossi da questi Ti sono quelli che minimizzano quella funzione
si itera, si parte da T0 per costruire gli alberi, Ti si costruisce a partire da Ti-1 rimuovendo un sott'albero e sostituendolo con una foglia con associata etichetta di maggioranza(in pratica l'etichetta che si presenta di più, non capisco però se deve essere un'attributo qualsiasi o un attributo classificatore), in pratica il sott'albero da rimuovere a Ti è quello che minimizza quella funzione(sulle slide)




a partire dall'albero decisionale è possibile estrarre regole del tipo if-then, per ogni cammino, i test che trovo vengono legati con &&, i valori nelle foglie sono le predizioni finali della classe, le regole sono tra di loro mutualmente esclusive(solo una regola può essere attiva alla volta) ed esaustive(ogni combinazione di attributi ha un esito, non proprio in realtà dato che potremmo classificare da meno attributi rispetto a quelli posseduti dalla tupla)
possiamo vedere la qualità di una regola dalla coverage(quante tuple, globali se ho ben capito, vengono coperte dalla regola) ed accuratezza(quante tuple di una certa classe coperte dalla regola)
per capire come definire queste regole definiamo gli esempi positivi e negativi(della stessa classe e non, anche se sulle slide specifica che siano della stessa classe, non saprei)
le regole nella stessa classe devono avere pochissimi overlap(intersezioni tra regole nella stessa classe, ma anche con classi esterne)

sono problemi di copertura minimale(minimal set covering), come per la normalizzazione infatti, con regole diverse ma il fine è lo stesso(trovare l'insieme minimo di regole con sufficentemente alto grado di accuratezza), dobbiamo trovare il set minimo di regole che riesca a coprire tutto(completo ed esaustivo se non mi sbaglio), sulle slide dice soltanto gli esempi positivi
per apprendere le regole usiamo un criterio greedy(ci sono anche altri modi ovviamente)

while(abbastanza tuple target da classificare){
	genera una regola;
	rimuovi le tuple target positive alla regola
}

più condizioni rendono la regola più specifica(diminuisce l'insieme di esempi positivi ma anche quella di insiemi negativi, diminuisce la coverage ma aumenta l'accuratezza)
foilgain e ripper per usare il nostro algoritmo greedy(da quel che ho capito ma da rivedere)

cerchiamo di bilanciare la coverage e l'accuratezza.



CLASSIFICATORI GENERATIVI:
sono classificatori che a partire dai dati costruiscono un modello probabilistico, si basano su bayes-chan, guarderemo naive bayes.

supponiamo un campione X, tupla, di cui non conosco la classe di appartenenza.
supponiamo l'ipotesi(Hc) che x appartenga alla classe C.
l'obiettivo è calcolare la prob dell'ipotesi data la variabile X, chiamata anche probabilità a posteriori.

evidenza, prob di osservare X
probabilità a priori, probabilità dell'ipotesi, calcolata dividendo le tuple che soddisfano l'ipotesi per tutti i membri della stessa classe.
likelihood, prob di osservare X data l'ipotesi(non mi convince, anche se come concetto effettivamente è quello, no il concetto è quello, quello che ti ricordavi tu era la loglikelihood, la likelihood è semplicemente la probabilitàa posteriori)
comunque nel naive bayes si suppone che gli attributi siano stocaticamente indipendenti(condizionalmente indipendenti), cioè la prob condizionata di X(variabile aleatoria multidimensionale) dato Hc(l'appartenenza alla classe) è uguale al prodotto di tutte le probabilità condizionate per ogni attributo(produttoria P(xi|Hc))
attenzione ad underflow e zero prob, si usa il loglikelihood per ovviare a tutti e due i problemi.
una cosa problematica del naive bayes è che non considera le dipendenze tra le variabili, per tentare di risolvere questo problema si possono utilizzare delle coppie o delle nuple di variabili
tutti gli svantaggi sono dovuti infatti all'assunzione dell'indipendenza tra gli attributi
i vantaggi sono dovuti al fatto che è veloce e facile da implementare, e spesso produce buoni risultati

CLASSIFICATORI DISCRIMINATIVI:
cercano di predire la classe direttamente a partire dai dati osservati, diverso dagli altri(costruivano un modello dai dati per predire le classi), a ma vabbeh i perceptron sono quelli delle reti neurali.
pesi per ogni attributo, difficilmente interpretabili.
discriminativi perchè penso favoriscono certi attributi e bloccano altri.
può essere lineare o non, cioè combinazione lineare degli attributi per i pesi, oppure combinazioni non lineari.
può essere binaria o non binaria, costruire un iperpiano che separa le due classi nel caso binario.
se esiste un iperpiano che divide i dati in due settori, diremo che i dati sono linearmente separabili.
perceptron calcola una funzione lineare.
apprendimento online, ad ogni nuovo record si aggiornano i pesi


le tuple arrivano un alla volta, come uno stream, osserviamo la tupla e la classe di appartenenza, aggiorniamo i pesi.
vabeh multilayer con hidden layer etc per le reti neurali.
se i dati sono linearmente separabili allora l'algoritmo perceptron converge